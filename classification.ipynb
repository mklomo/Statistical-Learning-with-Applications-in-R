{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "This data set consists of percentage returns for the S&P 500 stock index over 1, 250 days, from the\n",
    "beginning of 2001 until the end of 2005. For each date, we have recorded the percentage returns for each of the five previous trading days, Lag1 through Lag5.  Volume (the number of shares traded on the previous day, in billions), Today (the percentage return on the date in question) and Direction (whether the market was Up or Down on this\n",
    "date). Our goal is to predict Direction (a qualitative response) using the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Year'</li>\n",
       "\t<li>'Lag1'</li>\n",
       "\t<li>'Lag2'</li>\n",
       "\t<li>'Lag3'</li>\n",
       "\t<li>'Lag4'</li>\n",
       "\t<li>'Lag5'</li>\n",
       "\t<li>'Volume'</li>\n",
       "\t<li>'Today'</li>\n",
       "\t<li>'Direction'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Year'\n",
       "\\item 'Lag1'\n",
       "\\item 'Lag2'\n",
       "\\item 'Lag3'\n",
       "\\item 'Lag4'\n",
       "\\item 'Lag5'\n",
       "\\item 'Volume'\n",
       "\\item 'Today'\n",
       "\\item 'Direction'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Year'\n",
       "2. 'Lag1'\n",
       "3. 'Lag2'\n",
       "4. 'Lag3'\n",
       "5. 'Lag4'\n",
       "6. 'Lag5'\n",
       "7. 'Volume'\n",
       "8. 'Today'\n",
       "9. 'Direction'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n",
       "[7] \"Volume\"    \"Today\"     \"Direction\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(ISLR2)\n",
    "\n",
    "\n",
    "# Preview of Columns\n",
    "names(Smarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1250</li>\n",
       "\t<li>9</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1250\n",
       "\\item 9\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1250\n",
       "2. 9\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1250    9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dimensions of Dataset\n",
    "\n",
    "dim(Smarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Year           Lag1                Lag2                Lag3          \n",
       " Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n",
       " 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n",
       " Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n",
       " Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n",
       " 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n",
       " Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n",
       "      Lag4                Lag5              Volume           Today          \n",
       " Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n",
       " 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n",
       " Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n",
       " Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n",
       " 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n",
       " Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n",
       " Direction \n",
       " Down:602  \n",
       " Up  :648  \n",
       "           \n",
       "           \n",
       "           \n",
       "           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary of the dataset\n",
    "\n",
    "summary(Smarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Year</th><th scope=col>Lag1</th><th scope=col>Lag2</th><th scope=col>Lag3</th><th scope=col>Lag4</th><th scope=col>Lag5</th><th scope=col>Volume</th><th scope=col>Today</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Year</th><td>1.00000000  </td><td> 0.029699649</td><td> 0.030596422</td><td> 0.033194581</td><td> 0.035688718</td><td> 0.029787995</td><td> 0.53900647 </td><td> 0.030095229</td></tr>\n",
       "\t<tr><th scope=row>Lag1</th><td>0.02969965  </td><td> 1.000000000</td><td>-0.026294328</td><td>-0.010803402</td><td>-0.002985911</td><td>-0.005674606</td><td> 0.04090991 </td><td>-0.026155045</td></tr>\n",
       "\t<tr><th scope=row>Lag2</th><td>0.03059642  </td><td>-0.026294328</td><td> 1.000000000</td><td>-0.025896670</td><td>-0.010853533</td><td>-0.003557949</td><td>-0.04338321 </td><td>-0.010250033</td></tr>\n",
       "\t<tr><th scope=row>Lag3</th><td>0.03319458  </td><td>-0.010803402</td><td>-0.025896670</td><td> 1.000000000</td><td>-0.024051036</td><td>-0.018808338</td><td>-0.04182369 </td><td>-0.002447647</td></tr>\n",
       "\t<tr><th scope=row>Lag4</th><td>0.03568872  </td><td>-0.002985911</td><td>-0.010853533</td><td>-0.024051036</td><td> 1.000000000</td><td>-0.027083641</td><td>-0.04841425 </td><td>-0.006899527</td></tr>\n",
       "\t<tr><th scope=row>Lag5</th><td>0.02978799  </td><td>-0.005674606</td><td>-0.003557949</td><td>-0.018808338</td><td>-0.027083641</td><td> 1.000000000</td><td>-0.02200231 </td><td>-0.034860083</td></tr>\n",
       "\t<tr><th scope=row>Volume</th><td>0.53900647  </td><td> 0.040909908</td><td>-0.043383215</td><td>-0.041823686</td><td>-0.048414246</td><td>-0.022002315</td><td> 1.00000000 </td><td> 0.014591823</td></tr>\n",
       "\t<tr><th scope=row>Today</th><td>0.03009523  </td><td>-0.026155045</td><td>-0.010250033</td><td>-0.002447647</td><td>-0.006899527</td><td>-0.034860083</td><td> 0.01459182 </td><td> 1.000000000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllll}\n",
       "  & Year & Lag1 & Lag2 & Lag3 & Lag4 & Lag5 & Volume & Today\\\\\n",
       "\\hline\n",
       "\tYear & 1.00000000   &  0.029699649 &  0.030596422 &  0.033194581 &  0.035688718 &  0.029787995 &  0.53900647  &  0.030095229\\\\\n",
       "\tLag1 & 0.02969965   &  1.000000000 & -0.026294328 & -0.010803402 & -0.002985911 & -0.005674606 &  0.04090991  & -0.026155045\\\\\n",
       "\tLag2 & 0.03059642   & -0.026294328 &  1.000000000 & -0.025896670 & -0.010853533 & -0.003557949 & -0.04338321  & -0.010250033\\\\\n",
       "\tLag3 & 0.03319458   & -0.010803402 & -0.025896670 &  1.000000000 & -0.024051036 & -0.018808338 & -0.04182369  & -0.002447647\\\\\n",
       "\tLag4 & 0.03568872   & -0.002985911 & -0.010853533 & -0.024051036 &  1.000000000 & -0.027083641 & -0.04841425  & -0.006899527\\\\\n",
       "\tLag5 & 0.02978799   & -0.005674606 & -0.003557949 & -0.018808338 & -0.027083641 &  1.000000000 & -0.02200231  & -0.034860083\\\\\n",
       "\tVolume & 0.53900647   &  0.040909908 & -0.043383215 & -0.041823686 & -0.048414246 & -0.022002315 &  1.00000000  &  0.014591823\\\\\n",
       "\tToday & 0.03009523   & -0.026155045 & -0.010250033 & -0.002447647 & -0.006899527 & -0.034860083 &  0.01459182  &  1.000000000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Year | Lag1 | Lag2 | Lag3 | Lag4 | Lag5 | Volume | Today |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| Year | 1.00000000   |  0.029699649 |  0.030596422 |  0.033194581 |  0.035688718 |  0.029787995 |  0.53900647  |  0.030095229 |\n",
       "| Lag1 | 0.02969965   |  1.000000000 | -0.026294328 | -0.010803402 | -0.002985911 | -0.005674606 |  0.04090991  | -0.026155045 |\n",
       "| Lag2 | 0.03059642   | -0.026294328 |  1.000000000 | -0.025896670 | -0.010853533 | -0.003557949 | -0.04338321  | -0.010250033 |\n",
       "| Lag3 | 0.03319458   | -0.010803402 | -0.025896670 |  1.000000000 | -0.024051036 | -0.018808338 | -0.04182369  | -0.002447647 |\n",
       "| Lag4 | 0.03568872   | -0.002985911 | -0.010853533 | -0.024051036 |  1.000000000 | -0.027083641 | -0.04841425  | -0.006899527 |\n",
       "| Lag5 | 0.02978799   | -0.005674606 | -0.003557949 | -0.018808338 | -0.027083641 |  1.000000000 | -0.02200231  | -0.034860083 |\n",
       "| Volume | 0.53900647   |  0.040909908 | -0.043383215 | -0.041823686 | -0.048414246 | -0.022002315 |  1.00000000  |  0.014591823 |\n",
       "| Today | 0.03009523   | -0.026155045 | -0.010250033 | -0.002447647 | -0.006899527 | -0.034860083 |  0.01459182  |  1.000000000 |\n",
       "\n"
      ],
      "text/plain": [
       "       Year       Lag1         Lag2         Lag3         Lag4        \n",
       "Year   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718\n",
       "Lag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911\n",
       "Lag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533\n",
       "Lag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036\n",
       "Lag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000\n",
       "Lag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641\n",
       "Volume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246\n",
       "Today  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527\n",
       "       Lag5         Volume      Today       \n",
       "Year    0.029787995  0.53900647  0.030095229\n",
       "Lag1   -0.005674606  0.04090991 -0.026155045\n",
       "Lag2   -0.003557949 -0.04338321 -0.010250033\n",
       "Lag3   -0.018808338 -0.04182369 -0.002447647\n",
       "Lag4   -0.027083641 -0.04841425 -0.006899527\n",
       "Lag5    1.000000000 -0.02200231 -0.034860083\n",
       "Volume -0.022002315  1.00000000  0.014591823\n",
       "Today  -0.034860083  0.01459182  1.000000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pairwise Correlation on all numeric data\n",
    "\n",
    "cor(Smarket[, -9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations between the lag variables and today’s returns are close to zero. There appears to be little\n",
    "correlation between today’s returns and previous days’ returns. The only substantial correlation is between Year and Volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following objects are masked from Smarket (pos = 9):\n",
      "\n",
      "    Direction, Lag1, Lag2, Lag3, Lag4, Lag5, Today, Volume, Year\n",
      "\n",
      "The following objects are masked from Smarket (pos = 11):\n",
      "\n",
      "    Direction, Lag1, Lag2, Lag3, Lag4, Lag5, Today, Volume, Year\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing increasing Volume\n",
    "\n",
    "attach(Smarket)\n",
    "plot(Volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Up</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Down</th><td>0</td></tr>\n",
       "\t<tr><th scope=row>Up</th><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "  & Up\\\\\n",
       "\\hline\n",
       "\tDown & 0\\\\\n",
       "\tUp & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Up |\n",
       "|---|---|\n",
       "| Down | 0 |\n",
       "| Up | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "     Up\n",
       "Down 0 \n",
       "Up   1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify the Implicitly coded directions\n",
    "contrasts(Smarket$Direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "\n",
    "In general, we can perform a regression by modeling the response Y as coming from a particular member of the exponential family (Gaussian, Poisson, Bernoulli), and then transforming the mean of the response so that the transformed mean is a linear function of the predictors. Any regression approach that follows this very general recipe is known as a generalized linear model (GLM). Thus, linear regression, logistic generalized regression, and Poisson regression are three examples of GLMs. Other examples include Gamma regression and negative binomial regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n",
       "    Volume, family = binomial, data = Smarket)\n",
       "\n",
       "Deviance Residuals: \n",
       "   Min      1Q  Median      3Q     Max  \n",
       "-1.446  -1.203   1.065   1.145   1.326  \n",
       "\n",
       "Coefficients:\n",
       "             Estimate Std. Error z value Pr(>|z|)\n",
       "(Intercept) -0.126000   0.240736  -0.523    0.601\n",
       "Lag1        -0.073074   0.050167  -1.457    0.145\n",
       "Lag2        -0.042301   0.050086  -0.845    0.398\n",
       "Lag3         0.011085   0.049939   0.222    0.824\n",
       "Lag4         0.009359   0.049974   0.187    0.851\n",
       "Lag5         0.010313   0.049511   0.208    0.835\n",
       "Volume       0.135441   0.158360   0.855    0.392\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 1731.2  on 1249  degrees of freedom\n",
       "Residual deviance: 1727.6  on 1243  degrees of freedom\n",
       "AIC: 1741.6\n",
       "\n",
       "Number of Fisher Scoring iterations: 3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Running a logistic regression using the generalized linear model (glm)\n",
    "\n",
    "glm_fit <- glm(\n",
    "    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, \n",
    "    data = Smarket,\n",
    "    family = binomial\n",
    ")\n",
    "\n",
    "summary(glm_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smallest p-value here is associated with Lag1. The negative coefficient for this predictor suggests that if the market had a positive return yesterday, then it is less likely to go up today. However, at a value of 0.15, the p-value is still relatively large, and so there is no clear evidence of a real association between Lag1 and Direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with Logistic Regression Model\n",
    "\n",
    "The predict() function can be used to predict the probability that the market will go up, given values of the predictors. The type = \"response\" option tells R to output probabilities of the form P(Y = 1|X), as opposed to other information such as the logit. If no dataset is supplied, predict() function, then the probabilities are computed for the training data that was used to fit the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>1</dt>\n",
       "\t\t<dd>0.507084133395402</dd>\n",
       "\t<dt>2</dt>\n",
       "\t\t<dd>0.481467878454591</dd>\n",
       "\t<dt>3</dt>\n",
       "\t\t<dd>0.481138835214201</dd>\n",
       "\t<dt>4</dt>\n",
       "\t\t<dd>0.515222355813022</dd>\n",
       "\t<dt>5</dt>\n",
       "\t\t<dd>0.510781162691538</dd>\n",
       "\t<dt>6</dt>\n",
       "\t\t<dd>0.506956460534911</dd>\n",
       "\t<dt>7</dt>\n",
       "\t\t<dd>0.492650874187038</dd>\n",
       "\t<dt>8</dt>\n",
       "\t\t<dd>0.509229158207377</dd>\n",
       "\t<dt>9</dt>\n",
       "\t\t<dd>0.517613526170958</dd>\n",
       "\t<dt>10</dt>\n",
       "\t\t<dd>0.488837779771376</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[1] 0.507084133395402\n",
       "\\item[2] 0.481467878454591\n",
       "\\item[3] 0.481138835214201\n",
       "\\item[4] 0.515222355813022\n",
       "\\item[5] 0.510781162691538\n",
       "\\item[6] 0.506956460534911\n",
       "\\item[7] 0.492650874187038\n",
       "\\item[8] 0.509229158207377\n",
       "\\item[9] 0.517613526170958\n",
       "\\item[10] 0.488837779771376\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "1\n",
       ":   0.5070841333954022\n",
       ":   0.4814678784545913\n",
       ":   0.4811388352142014\n",
       ":   0.5152223558130225\n",
       ":   0.5107811626915386\n",
       ":   0.5069564605349117\n",
       ":   0.4926508741870388\n",
       ":   0.5092291582073779\n",
       ":   0.51761352617095810\n",
       ":   0.488837779771376\n",
       "\n"
      ],
      "text/plain": [
       "        1         2         3         4         5         6         7         8 \n",
       "0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 0.4926509 0.5092292 \n",
       "        9        10 \n",
       "0.5176135 0.4888378 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prediction with Logistic Regression Model\n",
    "\n",
    "glm_probs <- predict(glm_fit, type = \"response\")\n",
    "\n",
    "glm_probs[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, Up or Down. The following two commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Direction\n",
       "glm_pred Down  Up\n",
       "    Down  145 141\n",
       "    Up    457 507"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a Default Vector of Down with length = number\n",
    "# of rows in the Smarket Dataset\n",
    "glm_pred <- rep(\"Down\", nrow(Smarket))\n",
    "\n",
    "# For Probabilites > 0.5, pred = \"Up\"\n",
    "glm_pred[glm_probs > 0.5] = \"Up\"\n",
    "\n",
    "# Create a confusion matrix from the above\n",
    "table(glm_pred, Direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. Hence our model correctly predicted that the market would go up on 507 days and that it would go down on 145 days, for a total of 507 + 145 = 652 correct predictions. The mean() function can be used to compute the fraction of days for which the prediction was correct. In this case, logistic regression correctly predicted the movement of the market 52.2% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.5216"
      ],
      "text/latex": [
       "0.5216"
      ],
      "text/markdown": [
       "0.5216"
      ],
      "text/plain": [
       "[1] 0.5216"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(glm_pred == Direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, it appears that the logistic regression model is working a little better than random guessing. However, this result is misleading because we trained and tested the model on the same set of 1, 250 observations. In other words, *100% − 52.2% = 47.8%*, is the training error rate. The training error rate is often overly optimistic—it tends to underestimate the test error rate. In order to better assess the accuracy of the logistic regression model in this setting, we can fit the model using part of the data, and then examine how well it predicts the **held out** data.\n",
    "\n",
    "To implement this strategy, we will first create a vector corresponding to the observations from 2001 through 2004. We will then use this vector to create a held out data set of observations from 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat a boolean filter using the Year Colum\n",
    "train_index <- (Year < 2005)\n",
    "\n",
    "# Select the Test Predictors \n",
    "smarket_test <- Smarket[!train_index,]\n",
    "\n",
    "# Select the test Response\n",
    "direction_test <- Direction[!train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a logistic regression on the training subset\n",
    "glm_fit2 <- glm(\n",
    "    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, \n",
    "    data = Smarket,\n",
    "    family = binomial,\n",
    "    subset = train_index\n",
    ")\n",
    "\n",
    "glm_probs2 <- predict(glm_fit2, smarket_test, type = \"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have trained and tested our model on two completely separate data sets: training was performed using only the dates before 2005, and testing was performed using only the dates in 2005. Finally, we compute the predictions for 2005 and compare them to the actual movements of the market over that time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         direction_test\n",
       "glm_pred2 Down Up\n",
       "     Down   77 97\n",
       "     Up     34 44"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Default Resonse\n",
    "glm_pred2 <- rep(\"Down\", nrow(smarket_test))\n",
    "\n",
    "# Now, if the predicted probability is greater than 0.5, Response = \"Up\"\n",
    "glm_pred2[(glm_probs2 > 0.5)] = \"Up\"\n",
    "\n",
    "# Output the Confusion Matrix\n",
    "table(glm_pred2, direction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.51984126984127"
      ],
      "text/latex": [
       "0.51984126984127"
      ],
      "text/markdown": [
       "0.51984126984127"
      ],
      "text/plain": [
       "[1] 0.5198413"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find the Test error tate\n",
    "mean(glm_pred2 != direction_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are rather disappointing: the test error rate is 52 %, which is worse than random guessing! Of course this result\n",
    "is not all that surprising, given that one would not generally expect to be able to use previous days’ returns to predict future market performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We recall that the logistic regression model had very underwhelming pvalues associated with all of the predictors, and that the smallest p-value, though not very small, corresponded to Lag1. Perhaps by removing the variables that appear not to be helpful in predicting Direction, we can obtain a more effective model. After all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         direction_test\n",
       "glm_pred3 Down  Up\n",
       "     Down   35  35\n",
       "     Up     76 106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.44047619047619"
      ],
      "text/latex": [
       "0.44047619047619"
      ],
      "text/markdown": [
       "0.44047619047619"
      ],
      "text/plain": [
       "[1] 0.4404762"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform a logistic regression on the training subset\n",
    "glm_fit3 <- glm(\n",
    "    Direction ~ Lag1 + Lag2, \n",
    "    data = Smarket,\n",
    "    family = binomial,\n",
    "    subset = train_index\n",
    ")\n",
    "\n",
    "glm_probs3 <- predict(glm_fit3, smarket_test, type = \"response\")\n",
    "\n",
    "# Default Resonse\n",
    "glm_pred3 <- rep(\"Down\", nrow(smarket_test))\n",
    "\n",
    "# Now, if the predicted probability is greater than 0.5, Response = \"Up\"\n",
    "glm_pred3[(glm_probs3 > 0.5)] = \"Up\"\n",
    "\n",
    "# Output the Confusion Matrix\n",
    "table(glm_pred3, direction_test)\n",
    "\n",
    "# Find the Test error tate\n",
    "mean(glm_pred3 != direction_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the results appear to be a little better: 56% of the daily movements have been correctly predicted. It is worth noting that in this case, a much simpler strategy of predicting that the market will increase every day will also be correct 56% of the time ( 106 / (106 + 76) )! Hence, in terms of overall error rate, the logistic regression method is no better than the naive approach. However, the confusion matrix shows that on days when logistic regression predicts an increase in the market, it has a 58% accuracy rate ((76 + 106)/(76+106+35+35)). This suggests a possible trading strategy of buying on days when the model predicts an increasing market, and avoiding trades on days when a decrease is predicted must be investigated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>1</dt>\n",
       "\t\t<dd>0.479146239171912</dd>\n",
       "\t<dt>2</dt>\n",
       "\t\t<dd>0.496093872956532</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[1] 0.479146239171912\n",
       "\\item[2] 0.496093872956532\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "1\n",
       ":   0.4791462391719122\n",
       ":   0.496093872956532\n",
       "\n"
      ],
      "text/plain": [
       "        1         2 \n",
       "0.4791462 0.4960939 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prediction with new Data\n",
    "\n",
    "predict(glm_fit3, \n",
    "newdata = data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)),\n",
    "type = \"response\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "\n",
    "We fit an LDA model using the function, which is part of the *MASS* library. Notice that the syntax for the lda() function is identical to that of lm(), and to that of glm() except for the absence of the **family** option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train_index)\n",
       "\n",
       "Prior probabilities of groups:\n",
       "    Down       Up \n",
       "0.491984 0.508016 \n",
       "\n",
       "Group means:\n",
       "            Lag1        Lag2\n",
       "Down  0.04279022  0.03389409\n",
       "Up   -0.03954635 -0.03132544\n",
       "\n",
       "Coefficients of linear discriminants:\n",
       "            LD1\n",
       "Lag1 -0.6420190\n",
       "Lag2 -0.5135293"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(MASS)\n",
    "\n",
    "lda_fit <- lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train_index)\n",
    "\n",
    "lda_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA output indicates that ˆπ1 = 0.492 and ˆπ2 = 0.508; in other words, 49.2% of the training observations correspond to days during which the market went down. It also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of μk. These suggest that there is a tendency for the previous 2 days’ returns to be negative on days when the market increases, and a tendency for the previous days’ returns to be positive on days when the market declines. The\n",
    "coefficients of linear discriminants output provides the linear combination of Lag1 and Lag2 that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of X = x in (4.24). If −0.642 × Lag1 − 0.514 × Lag2 is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a\n",
    "market decline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAAAA//9NTU1oaGh8fHyMjIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD////ZpP2iAAAACXBIWXMAABJ0AAASdAHeZh94AAAZq0lEQVR4nO3d60LiShaA0UwAUZHL+z/tSEAb22MLYYfaVVnrxwyHa1HJ10BCpDsAd+tKDwBaICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIMFFI3aVpHgISecBaLqQxuiuUHiN/CCmp7n+/MrGJCCkpIdVFSEkJqS5CSkpIdRFSUkKqi5CSElJdhJSUkOoipKSEVBchJSWkuggpKSHVRUhJCakuQkpKSHURUlJCqouQkhJSXYSUlJDqIqSkhFQXISUlpLoIKSkh1UVISQmpLkJKSkh1EVJSQqqLkJISUl3uXxi//lUoy3sMIdVFSEkJqS5jF8YNf6rQ8h5DSHUZuzDeeiFNSkh1Gb0w9qtuuRvuwVu7KQipLncsjNeuez0IaSJCqss9C2O37FZ7IU1DSHW5b2E8d/1GSJMQUl3uXBjbxe+/LmJ5jyGkuty9MJ6ENAkh1cVXhJISUl2ElJSQ6hKyMOyQjSekukwUkp86vZeQ6uKtXVLXhOQHm/MQUlLXhPT7VbxqPYqQkhJSXcZP9NvzanjvsFq/TfUQcyakuoyd6P3i4n34cpKHmDch1WXsRK+7/nU7nNpt+m49xUPMm5DqMnai+277eXrb9VM8xLwJqS7jDzX/6T/CHmLehFQXr0hJCakud3xG2gxHmvuMNA0h1WX0RC8vttot9pM8xKwJqS537EdaD/uR+tWz/UgTEFJdfLMhKSHVRUhJCakuQkpKSHURUlJCqouQkhJSXYSUlJDqIqSkhFQXISUlpLoIKSkh1UVISQmpLkJKSkh1EVJSQqqLkJISUl2ElJSQ6iKkpIRUFyElJaS6CKmAq/5ot5CqIqQCripASFURUgFCao+QChBSe4RUgJDaI6QChNQeIRUgpPYIqQAhtWeiifYzpv8ipPZ4RSpASO0RUgFCao+QChBSe4RUgJDaI6QChNQeIRUgpPYIqQAhtUdIBQipPUIq4JEhXaP0fLRASAU8MqRrrlN6PlogpAKE1B4hFSCk9gipACG1R0gFCKk9QipASO0RUgFCao+QChBSe4RUgJDaI6QChNQeIRUgpPYIqQAhtUdIBQipPUIqQEjtGT2J+6euW27Od/LPe7Gc/iak9oydxH0/HMmyOt2JkG4ipPaMncR19/Je00u/HO5ESDcRUnvGTmJ/uuGuX+yEdCshtWfsJH60s18uhXQrIbVn7CQuuv3HqaWQbiSk9oydxJfu6Xxq1y2FdBshtWf0JK4/69n88ndoLKe/Cak94ydxu/o4tXsS0k2E1B7fbChASO0RUgFCao+QChBSe0Im0caG2wipPROF5E9L/4uQ2uOtXQFCao+QChBSe4RUgJDaM34S355Xp0OS1m9TPUSrhNSe0Qf2LS62JiwneYh2Cak94w/s61+3w6ndpu/WUzxEu4TUnvEH9m0/T2+7foqHaJeQ2nPvgX3f/yPsIdolpPZ4RSpASO254zPSZjec8hnpZkJqz+hJXF5stVvs/3VNy+lvQmrPHfuR1sN+pH71bD/SjYTUHt9sKEBI7RFSAUJqj5AKEFJ7hFRAtpCuUHrK0hNSAdlCuuIqpacsPSEVIKT2CKkAIbVHSAUIqT1CKkBI7RFSAUJqj5AKEFJ7hFSAkNojpAKE1B4hFSCk9gipACG1R0gFCKk9QipASO0RUgFCao+QChBSe4QU7Zqje4LW7pi7EVIEIUV7XAFCSkRI0YQ0S0KKJqRZElI0Ic2SkKIJaZYmmqEZ/wUaIc2SV6RoQpolIUUT0iwJKZqQZklI0YQ0S0KKJqRZElI0Ic2SkKIJaZaEFE1IsySkaEKaJSFFE9IsCSmakGZJSNGENEtCiiakWRLSLa75ewxCmiUh3SJXAUJKREi3yFWAkBIR0qfq/o6WkBIR0qfqChBSIkL6VF0BQkpESJ+qK+CRIV2j9AIsSkifqivgkSFdczelF2BRQvpUXQFCSkRIn6orQEiJjH/2b8+r4Z3xav021UM8VnUFCCmRsc9+v7j4lLmc5CEerboChJTI2Ge/7vrX7XBqt+m79RQP8WjVFSCkRMY++77bfp7edv0UD/Fo1RUgpETGPvsvew3+vQuhlgmurgAhJTKTV6Qmj3/IFtKs99ne8RlpsxtOVfEZKdeqm+tuHjni0uvBdEY/teXFvzOL/SQPESjZ+pTqbh454nZfte7Yj7Qe9iP1q+cK9iMlW59S3U22EZdeV8aZyTcbKlyfHnY32UZcel0ZR0jhK0Jtd5NtxKXXlXFaCClmi1y29elhd5NtxI8TuhKG3EnZ/UjJVoTa7qa+EWfchjhRSFeF/8B/e+C7iHX/c2WOvDOYKyFBACFBgAcc2Afte8CBfdC+BxzYB+17wGEUdymzXZT6TbZK/rCijr3d9Qf23SXXxpBco0k2nHmPJv0r0mT3PEau0SQbzrxH84AD++4y76Xzi1zDmfdoHnBg313mvXR+kWs48x7NAw7su8u8l84vcg1n3qPJ9ey/yzW+XKNJNpx5jybXs/8u1/hyjSbZcOY9mlzP/rtc48s1mmTDmfdocj3773KNL9dokg1n3qPJ9ey/yzW+XKNJNpx5jybXs/8u1/hyjSbZcOY9mlzP/rtc48s1mmTDmfdocj17qJSQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIEANIb3lGeTLouvX0/05zFus+zRDOaSamJNHrzR51tEf7fs0g1wPf1i2z7DCnP7S7aL0MM4STczJw1eaNOvoz1YP/4mOn2y7p/dV5aV7Kj2Q4z+4/faw7bscv5aYaGLOHr7SZFlHf/b6+N+6+cnqNJAM41l3m8Nxbp5LD2SQaGJOHr/SpHnqP9l1yzzL5yTDeFbd8bdAtt2q9EAuZZiYQYGVJstT/9Gy26VZPif7DL+Z22V7DTgkmZhBgZUm04L4L8/da6615fhRYFN6CDlDSjExRyVWmkwL4j8M711SrS2HXZ/h7VTCkHJMzKHQSpNoQfyXxXGLaqa15bDvU7x/yRdSkok5FFpp8iyIL84/TP00vFkovrZc/kz2Mseumz5dSEkm5lBopcmzIL44r7rFfuz9P0fzbrdY7ooO5cNpq90uzVa7NBNzKLTSJA3pLElInzZptks9D//sbib8Geyb5JkYIf0sS0bH3ROlh/Ah1zcbEk3MB2/tvksT0lOi18fFMJAk62+miTkT0ndplk+mN5r74dvfpUdxlmlizoQEFRISBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBBS5U6/lLdY70sPZOaEVLmPH53sd6VHMm9Cqtzpt1J3yyy/yzxXQqrcx48OL7pN2YHMnJCyWPfdesii6/aLbvV+zsuiW7wcLzrFcrrssP76Y+YfIW26p8Of27yd/mtzquupe+u63arrnx/3dOZGSEksjx90nk6xrLr3pk7nDO/YLkN6/jjz7COkfbc4XNymH85+6obmuv79av3xEiVNRUg5bLp+e9j2p1iWx01wr+dzXr+G9Hnm2UdIw4k/t3kervJ+9eGenk93+jLExhSElMNqeBO2OcXydnnO8mtIpzNXnzf8EtKf2+yOt3t7f23bHl+mduc77SzuqZjZHM6r+Plz0H+e8/2yLyf/unjZ7d8/Tm3fX4x2X1tkEmY2h7tDushl+L/Ne0L94rBYnN7lCWliZjaHu0N6PW5XuLy4W7y9n7U+bgLcC2lyZjaHL5+RvpyzOp/z9ufz03lT9+DPfqS3L7d5T+jp/b/erzpcWUgTM7M5fNlqN5xzsdVu0b0c9svLrXZ/dr5++WbDxW2O4Z1eioYrC2liZjaJ5fk7c3/W9j/7kV6OJ1bnTePD6T+3+/pduz+3OeZ32rPUn67253+ZgJnNYt13y7fLkA4v/fmbDYfn/v392fmy1ceZJ6eMls/fbnN4HvbGPp/3yQppWmY2ld++eqqErCyYHLrj55r9qlv/drXHDIdbWTA5PJ8/6fxyNSFlZcEk8bI8Huf627WElJUFAwGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAEmCqm7NM1DQCIPWMuFRPuEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGEBAGElFQXpPTzmAshJdX971dXXOV/Jv9BhJSUkOoipKSEVJf7J/rXt+GW5RhCqouQkhJSXcZO9A3bhizLMYRUl7ET/dYLaVJCqsvoid6vuuVuuAdv7aYgpLrcMdGvXfd6ENJEhFSXeyZ6t+xWeyFNQ0h1uW+in7t+I6RJCKkud070dvH717ksyzGEVJe7J/pJSJMQUl18RSgpIdVFSEkJqS4hE22HbDwh1WWikBxbdi8h1cVbu6SEVBchJSWkuggpKSHVZfxEvz2vhk9Aq/XbVA8xZ0Kqy9iJ3i8utiYsJ3mIeRNSXcZO9LrrX7fDqd2m79ZTPMS8CakuYye677afp7ddP8VDzJuQ6jL+UPOf/iPsIeZNSHXxipSUkOpyx2ekzXCkuc9I0xBSXUZP9PJiq91iP8lDzJqQ6nLHfqT1sB+pXz3bjzQBIdXFNxuSElJdhJSUkOoipKSEVBchJSWkuggpKSHVRUhJCakuQkpKSHURUlJCqouQkhJSXYSUlJDqIqSkhFQXISUlpLoIKSkh1UVIBXTXEFJVhFTAVQUIqSpCKkBI7RFSAUJqj5AKEFJ7hFSAkNojpAKE1B4hFSCk9gipACG1R0gFCKk9QipASO2ZaKL9GPO/CKk9XpEKEFJ7hFTAI0O6Run5aIGQCnhkSNdcp/R8tEBIBQipPUIqQEjtEVIBQmqPkAoQUnuEVICQ2iOkAoTUHiEVIKT2CKkAIbVHSAUIqT1CKkBI7RFSAUJqj5AKEFJ7hFSAkNojpAKE1B4hFSCk9gipACG1R0gFCKk9QipASO0RUgFCao+QChBSe0ZP4v6p65ab8538814sp78JqT1jJ3HfD3/IaXW6EyHdREjtGTuJ6+7lvaaXfjnciZBuIqT2jJ3E/nTDXb/YCelWQmrP2En8aGe/XArpVkJqz9hJXHT7j1NLId1ISO0ZO4kv3dP51K5bCuk2QmrP6Elcf9az+eXPsFtOfxNSe8ZP4nb1cWr3JKSbCKk9vtlQgJDaI6QChNQeIRUgpPaETKKNDbcRUnsmCskvK/6LkNrjrV0BQmqPkAoQUnuEVICQ2jN+Et+eV6dDktZvUz1Eq4TUntEH9i0utiYsJ3mIdgmpPeMP7Otft8Op3abv1lM8RLuE1J7xB/ZtP09vu36Kh2iXkNpz74F93/8j7CHaJaT2eEUqQEjtueMz0mY3nPIZ6WZCas/oSVxebLVb7P91Tcvpb0Jqzx37kdbDfqR+9Ww/0o2E1B7fbChASO0RUgFCao+QChBSe4RUQLaQrlB6ytITUgHZQrriKqWnLD0hFSCk9gipACG1R0gFCKk9QipASO0RUgFCao+QChBSe4RUgJDaI6QChNQeIRUgpPYIqQAhtUdIBQipPUIqQEjtEVIBQmqPkAoQUnuEFO2ao3uC1u6YuxFSBCFFe1wBQkpESNGENEtCiiakWRJSNCHNkpCiCWmWJpqhGf8FGiHNklekaEKaJSFFE9IsCSmakGZJSNGENEtCiiakWRJSNCHNkpCiCWmWhBRNSLMkpGhCmiUhRRPSLAkpmpBmSUjR2gzJr/r9QkjR2gzpmrspPfNFCSmakGZJSNGENEtCiiakWRJSNCHNkpCiCWmWhHSLq7YCB62WD7sbIUUQ0i1yFSCkRIR0i1wFCCkRId0iVwFCSkRIt8hVgJASGf/s355Xw2fr1fptqodIJ1cBQkpk7LPfLy62Uy0neYiEchUgpETGPvt1179uh1O7Td+tp3iIhHIVIKRExj77vtt+nt52/RQPkVCuAoSUyNhn/+Xok38fitLQBOcqQEiJeEW6Ra4CsoU062P/7viMtNkNp3xGmma1fNjdPHLEpRfgdEY/teXFvzOL/SQP8WAx36MT0r+u0u6r1h37kdbDfqR+9Vx6P1LQ0qmugApDuuZu6mythW82BL2nSLY+Pexu6huxkP664IEvJdcIWcj1rZb1jVhIf11wzbRe8wAhS6fC9Snmbuob8VUrxYPfIYbc2cj9SEJKcTf1jThqpbjibq42UUhXhX/V+y2YSsS6/7kyR94ZzJWQIICQIMADDuyD9j3gwD5o3wMO7IP2PeAwiruU2S5K/SZbJX9YUcfe7voD++6Sa2NIrtEkG868R5P+FWmyex4j12iSDWfeo3nAgX13mffS+UWu4cx7NA84sO8u8146v8g1nHmP5gEH9t1l3kvnF7mGM+/R5Hr23+UaX67RJBvOvEeT69l/l2t8uUaTbDjzHk2uZ/9drvHlGk2y4cx7NLme/Xe5xpdrNMmGM+/R5Hr23+UaX67RJBvOvEeT69l/l2t8uUaTbDjzHk2uZ/9drvHlGk2y4cx7NLmePVRKSBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBBASBCghpDe8gzyZdH16+n+HOYt1n2aoRxSTczJo1eaPOvoj/Z9mkGuhz8s22dYYU5/6XZRehhniSbm5OErTZp19Gerh/9Ex0+23dP7qvLSPZUeyPEf3H572PZdjl9LTDQxZw9fabKsoz97ffxv3fxkdRpIhvGsu83hODfPpQcySDQxJ49fadI89Z/sumWe5XOSYTyr7vhbINtuVXoglzJMzKDASpPlqf9o2e3SLJ+TfYbfzO2yvQYckkzMoMBKk2lB/Jfn7jXX2nL8KLApPYScIaWYmKMSK02mBfEfhvcuqdaWw67P8HYqYUg5JuZQaKVJtCD+y+K4RTXT2nLY9ynev+QLKcnEHAqtNHkWxBfnH6Z+Gt4sFF9bLn8me5lj102fLqQkE3MotNLkWRBfnFfdYj/2/p+jebdbLHdFh/LhtNVul2arXZqJORRaaZKGdJYkpE+bNNulnod/djcT/gz2TfJMjJB+liWj4+6J0kP4kOubDYkm5oO3dt+lCekp0evjYhhIkvU308ScCem7NMsn0xvN/fDt79KjOMs0MWdCggoJCQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIICQIIqSGfv1KX6afzZsKMN0RI5ZjxhgipHDPeECGVY8ZTWvfdesih6/aLbvV+zsuiW7wcLzpFcrrssP7yy+ZfQvp2KVMSUkbL7t3TKYdV997U6Zxuefga0vPHmSd/hfTXpUxJSAltun572PanHJb793Nez+e8fg3p88yTv0L661KmJKSEVt3mcMxpyOHt8pzl15BOZ64+bvdXSH9dypSElNA5iPMnnf885/tlh28h/XUpUzLNCQmpPqY5obEhLbrd8P+7biGkBzPNCX35jPTlnNX5nLc/n5823dPH7Z66YQP54eV41rdLmZKQEvqy1W4452Kr3eK9lv3ycrvc5s/thpJeh4a+XcqUhJTRaa/RRUgX+5FejidW503jw+k/t1ufb3fcDfv9UiYkpJTWfbd8uwzp8NKfv9lweO7f366dL1t9nHm2WR3bGV6E/uNSpiOkvH77VsK/tyPYyvBQZjuh7vhZaL/qfvminJASMdsJPZ8+6fS/XE1IiZjtjF6WXbf49YvbQkrEbEMAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUEAIUGA/wNkYq6As08syQAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the lda_fit\n",
    "\n",
    "plot(lda_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot() function produces plots of the linear discriminants, obtained by computing −0.642×Lag1−0.514×Lag2 for each of the training observations. The Up and Down observations are displayed separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'class'</li>\n",
       "\t<li>'posterior'</li>\n",
       "\t<li>'x'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'class'\n",
       "\\item 'posterior'\n",
       "\\item 'x'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'class'\n",
       "2. 'posterior'\n",
       "3. 'x'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"class\"     \"posterior\" \"x\"        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform Prediction using the test data\n",
    "\n",
    "lda_pred <- predict(lda_fit, smarket_test)\n",
    "\n",
    "# Field names\n",
    "names(lda_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict() function returns a list with three elements. The first element, class, contains LDA’s predictions about the movement of the market. The second element, posterior, is a matrix whose kth column contains the posterior probability that the corresponding observation belongs to the kth class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         direction_test\n",
       "lda_class Down  Up\n",
       "     Down   35  35\n",
       "     Up     76 106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.55952380952381"
      ],
      "text/latex": [
       "0.55952380952381"
      ],
      "text/markdown": [
       "0.55952380952381"
      ],
      "text/plain": [
       "[1] 0.5595238"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the predicted classes from lda\n",
    "lda_class <- lda_pred$class \n",
    "\n",
    "# Building a confusion matrix\n",
    "table(lda_class, direction_test)\n",
    "\n",
    "# Get the means\n",
    "mean(lda_class == direction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "70"
      ],
      "text/latex": [
       "70"
      ],
      "text/markdown": [
       "70"
      ],
      "text/plain": [
       "[1] 70"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "182"
      ],
      "text/latex": [
       "182"
      ],
      "text/markdown": [
       "182"
      ],
      "text/plain": [
       "[1] 182"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Applying the same Posterior Probability cut-off\n",
    "sum(lda_pred$posterior[, 1] >= 0.5)\n",
    "\n",
    "sum(lda_pred$posterior[, 1] < 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to use a posterior probability threshold other than 50% in order to make predictions, then we could easily do so. For instance, suppose that we wish to predict a market decrease only if we are very certain that the market will indeed decrease on that day—say, if the posterior probability is at least 90 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0"
      ],
      "text/latex": [
       "0"
      ],
      "text/markdown": [
       "0"
      ],
      "text/plain": [
       "[1] 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(lda_pred$posterior[, 1] > 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that the posterior probability output by the model corresponds to the probability that the market will *decrease*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>lda_pred.posterior.1.20..1.</th><th scope=col>lda_class.1.20.</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>999</th><td>0.4901792</td><td>Up       </td></tr>\n",
       "\t<tr><th scope=row>1000</th><td>0.4792185</td><td>Up       </td></tr>\n",
       "\t<tr><th scope=row>1001</th><td>0.4668185</td><td>Up       </td></tr>\n",
       "\t<tr><th scope=row>1002</th><td>0.4740011</td><td>Up       </td></tr>\n",
       "\t<tr><th scope=row>1003</th><td>0.4927877</td><td>Up       </td></tr>\n",
       "\t<tr><th scope=row>1004</th><td>0.4938562</td><td>Up       </td></tr>\n",
       "\t<tr><th scope=row>1005</th><td>0.4951016</td><td>Up       </td></tr>\n",
       "\t<tr><th scope=row>1006</th><td>0.4872861</td><td>Up       </td></tr>\n",
       "\t<tr><th scope=row>1007</th><td>0.4907013</td><td>Up       </td></tr>\n",
       "\t<tr><th scope=row>1008</th><td>0.4844026</td><td>Up       </td></tr>\n",
       "\t<tr><th scope=row>1009</th><td>0.4906963</td><td>Up       </td></tr>\n",
       "\t<tr><th scope=row>1010</th><td>0.5119988</td><td>Down     </td></tr>\n",
       "\t<tr><th scope=row>1011</th><td>0.4895152</td><td>Up       </td></tr>\n",
       "\t<tr><th scope=row>1012</th><td>0.4706761</td><td>Up       </td></tr>\n",
       "\t<tr><th scope=row>1013</th><td>0.4744593</td><td>Up       </td></tr>\n",
       "\t<tr><th scope=row>1014</th><td>0.4799583</td><td>Up       </td></tr>\n",
       "\t<tr><th scope=row>1015</th><td>0.4935775</td><td>Up       </td></tr>\n",
       "\t<tr><th scope=row>1016</th><td>0.5030894</td><td>Down     </td></tr>\n",
       "\t<tr><th scope=row>1017</th><td>0.4978806</td><td>Up       </td></tr>\n",
       "\t<tr><th scope=row>1018</th><td>0.4886331</td><td>Up       </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & lda\\_pred.posterior.1.20..1. & lda\\_class.1.20.\\\\\n",
       "\\hline\n",
       "\t999 & 0.4901792 & Up       \\\\\n",
       "\t1000 & 0.4792185 & Up       \\\\\n",
       "\t1001 & 0.4668185 & Up       \\\\\n",
       "\t1002 & 0.4740011 & Up       \\\\\n",
       "\t1003 & 0.4927877 & Up       \\\\\n",
       "\t1004 & 0.4938562 & Up       \\\\\n",
       "\t1005 & 0.4951016 & Up       \\\\\n",
       "\t1006 & 0.4872861 & Up       \\\\\n",
       "\t1007 & 0.4907013 & Up       \\\\\n",
       "\t1008 & 0.4844026 & Up       \\\\\n",
       "\t1009 & 0.4906963 & Up       \\\\\n",
       "\t1010 & 0.5119988 & Down     \\\\\n",
       "\t1011 & 0.4895152 & Up       \\\\\n",
       "\t1012 & 0.4706761 & Up       \\\\\n",
       "\t1013 & 0.4744593 & Up       \\\\\n",
       "\t1014 & 0.4799583 & Up       \\\\\n",
       "\t1015 & 0.4935775 & Up       \\\\\n",
       "\t1016 & 0.5030894 & Down     \\\\\n",
       "\t1017 & 0.4978806 & Up       \\\\\n",
       "\t1018 & 0.4886331 & Up       \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | lda_pred.posterior.1.20..1. | lda_class.1.20. |\n",
       "|---|---|---|\n",
       "| 999 | 0.4901792 | Up        |\n",
       "| 1000 | 0.4792185 | Up        |\n",
       "| 1001 | 0.4668185 | Up        |\n",
       "| 1002 | 0.4740011 | Up        |\n",
       "| 1003 | 0.4927877 | Up        |\n",
       "| 1004 | 0.4938562 | Up        |\n",
       "| 1005 | 0.4951016 | Up        |\n",
       "| 1006 | 0.4872861 | Up        |\n",
       "| 1007 | 0.4907013 | Up        |\n",
       "| 1008 | 0.4844026 | Up        |\n",
       "| 1009 | 0.4906963 | Up        |\n",
       "| 1010 | 0.5119988 | Down      |\n",
       "| 1011 | 0.4895152 | Up        |\n",
       "| 1012 | 0.4706761 | Up        |\n",
       "| 1013 | 0.4744593 | Up        |\n",
       "| 1014 | 0.4799583 | Up        |\n",
       "| 1015 | 0.4935775 | Up        |\n",
       "| 1016 | 0.5030894 | Down      |\n",
       "| 1017 | 0.4978806 | Up        |\n",
       "| 1018 | 0.4886331 | Up        |\n",
       "\n"
      ],
      "text/plain": [
       "     lda_pred.posterior.1.20..1. lda_class.1.20.\n",
       "999  0.4901792                   Up             \n",
       "1000 0.4792185                   Up             \n",
       "1001 0.4668185                   Up             \n",
       "1002 0.4740011                   Up             \n",
       "1003 0.4927877                   Up             \n",
       "1004 0.4938562                   Up             \n",
       "1005 0.4951016                   Up             \n",
       "1006 0.4872861                   Up             \n",
       "1007 0.4907013                   Up             \n",
       "1008 0.4844026                   Up             \n",
       "1009 0.4906963                   Up             \n",
       "1010 0.5119988                   Down           \n",
       "1011 0.4895152                   Up             \n",
       "1012 0.4706761                   Up             \n",
       "1013 0.4744593                   Up             \n",
       "1014 0.4799583                   Up             \n",
       "1015 0.4935775                   Up             \n",
       "1016 0.5030894                   Down           \n",
       "1017 0.4978806                   Up             \n",
       "1018 0.4886331                   Up             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.frame(lda_pred$posterior[1:20, 1], lda_class[1:20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train_index)\n",
       "\n",
       "Prior probabilities of groups:\n",
       "    Down       Up \n",
       "0.491984 0.508016 \n",
       "\n",
       "Group means:\n",
       "            Lag1        Lag2\n",
       "Down  0.04279022  0.03389409\n",
       "Up   -0.03954635 -0.03132544"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the QDA model using the qda function from the MASS library\n",
    "qda_fit <- qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train_index)\n",
    "\n",
    "qda_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors. The predict() function works in exactly the same fashion as for LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         direction_test\n",
       "qda_class Down  Up\n",
       "     Down   30  20\n",
       "     Up     81 121"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.599206349206349"
      ],
      "text/latex": [
       "0.599206349206349"
      ],
      "text/markdown": [
       "0.599206349206349"
      ],
      "text/plain": [
       "[1] 0.5992063"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform predictions\n",
    "qda_class <- predict(qda_fit, smarket_test)$class\n",
    "\n",
    "# Get the Confusion Matrix\n",
    "table(qda_class, direction_test)\n",
    "\n",
    "mean(qda_class == direction_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the QDA predictions are accurate almost 60% of the time, even though the 2005 data was not used to fit the model. This level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurately. This suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression. However, we recommend evaluating this method’s performance on a larger test set before betting\n",
    "that this approach will consistently beat the market!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Naive Bayes is implemented in R using the naiveBayes() function, which is part of the e1071 library. The syntax is identical to that of lda() and qda(). By default, this implementation of the naive Bayes classifier models each quantitative feature using a Gaussian distribution. However, a kernel density method can also be used to estimate the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Naive Bayes Classifier for Discrete Predictors\n",
       "\n",
       "Call:\n",
       "naiveBayes.default(x = X, y = Y, laplace = laplace)\n",
       "\n",
       "A-priori probabilities:\n",
       "Y\n",
       "    Down       Up \n",
       "0.491984 0.508016 \n",
       "\n",
       "Conditional probabilities:\n",
       "      Lag1\n",
       "Y             [,1]     [,2]\n",
       "  Down  0.04279022 1.227446\n",
       "  Up   -0.03954635 1.231668\n",
       "\n",
       "      Lag2\n",
       "Y             [,1]     [,2]\n",
       "  Down  0.03389409 1.239191\n",
       "  Up   -0.03132544 1.220765\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(e1071)\n",
    "\n",
    "nb_fit <- naiveBayes(Direction ~ Lag1 + Lag2, data = Smarket,\n",
    "subset = train_index)\n",
    "\n",
    "nb_fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        direction_test\n",
       "nb_class Down  Up\n",
       "    Down   28  20\n",
       "    Up     83 121"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.591269841269841"
      ],
      "text/latex": [
       "0.591269841269841"
      ],
      "text/markdown": [
       "0.591269841269841"
      ],
      "text/plain": [
       "[1] 0.5912698"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prediction with Naive Bayes\n",
    "nb_class <- predict(nb_fit, smarket_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "table(nb_class, direction_test)\n",
    "\n",
    "# Accuracy\n",
    "mean(nb_class == direction_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Naive Bayes performs very well on this data, with accurate predictions over 59% of the time. This is slightly worse than QDA, but much better than LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Down</th><th scope=col>Up</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.4873164</td><td>0.5126836</td></tr>\n",
       "\t<tr><td>0.4762492</td><td>0.5237508</td></tr>\n",
       "\t<tr><td>0.4653377</td><td>0.5346623</td></tr>\n",
       "\t<tr><td>0.4748652</td><td>0.5251348</td></tr>\n",
       "\t<tr><td>0.4901890</td><td>0.5098110</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{ll}\n",
       " Down & Up\\\\\n",
       "\\hline\n",
       "\t 0.4873164 & 0.5126836\\\\\n",
       "\t 0.4762492 & 0.5237508\\\\\n",
       "\t 0.4653377 & 0.5346623\\\\\n",
       "\t 0.4748652 & 0.5251348\\\\\n",
       "\t 0.4901890 & 0.5098110\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Down | Up |\n",
       "|---|---|\n",
       "| 0.4873164 | 0.5126836 |\n",
       "| 0.4762492 | 0.5237508 |\n",
       "| 0.4653377 | 0.5346623 |\n",
       "| 0.4748652 | 0.5251348 |\n",
       "| 0.4901890 | 0.5098110 |\n",
       "\n"
      ],
      "text/plain": [
       "     Down      Up       \n",
       "[1,] 0.4873164 0.5126836\n",
       "[2,] 0.4762492 0.5237508\n",
       "[3,] 0.4653377 0.5346623\n",
       "[4,] 0.4748652 0.5251348\n",
       "[5,] 0.4901890 0.5098110"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying Prediction Probabilities\n",
    "\n",
    "nb_pred2 <- predict(nb_fit, smarket_test, type = \"raw\")\n",
    "\n",
    "# Display first 5 rows\n",
    "nb_pred2[1:5, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "\n",
    "We will now perform KNN using the knn() function, which is part of the class library. This function works rather differently from the other modelfitting functions that we have encountered thus far. Rather than a two-step approach in which we first fit the model and then we use the model to make predictions, knn() forms predictions using a single command.\n",
    "\n",
    "The function requires four inputs.\n",
    "1. A matrix containing the predictors associated with the training data,\n",
    "labeled train.X below.\n",
    "2. A matrix containing the predictors associated with the data for which\n",
    "we wish to make predictions, labeled test.X below.\n",
    "3. A vector containing the class labels for the training observations,\n",
    "labeled train.Direction below.\n",
    "4. A value for K, the number of nearest neighbors to be used by the\n",
    "classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(class)\n",
    "\n",
    "train_X <- cbind(Lag1, Lag2)[train_index, ]\n",
    "\n",
    "test_X <- cbind(Lag1, Lag2)[!train_index, ]\n",
    "\n",
    "train_direction <- Direction[train_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now the knn() function can be used to predict the market’s movement for the dates in 2005. We set a random seed before we apply knn() because if several observations are tied as nearest neighbors, then R will randomly break the tie. Therefore, a seed must be set in order to ensure reproducibility of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        direction_test\n",
       "knn_pred Down Up\n",
       "    Down   43 58\n",
       "    Up     68 83"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.5"
      ],
      "text/latex": [
       "0.5"
      ],
      "text/markdown": [
       "0.5"
      ],
      "text/plain": [
       "[1] 0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(1)\n",
    "\n",
    "# Prediction with knn\n",
    "knn_pred <- knn(train_X, test_X, train_direction, k = 1)\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "table(knn_pred, direction_test)\n",
    "\n",
    "# Rate\n",
    "mean(knn_pred == direction_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The results using K = 1 are not very good, since only 50% of the observations are correctly predicted. Of course, it may be that K = 1 results in an overly flexible fit to the data. Below, we repeat the analysis using K = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        direction_test\n",
       "knn_pred Down Up\n",
       "    Down   48 54\n",
       "    Up     63 87"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.535714285714286"
      ],
      "text/latex": [
       "0.535714285714286"
      ],
      "text/markdown": [
       "0.535714285714286"
      ],
      "text/plain": [
       "[1] 0.5357143"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prediction with knn\n",
    "knn_pred <- knn(train_X, test_X, train_direction, k = 3)\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "table(knn_pred, direction_test)\n",
    "\n",
    "# Rate\n",
    "mean(knn_pred == direction_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The results have improved slightly. But increasing K further turns out\n",
    "to provide no further improvements. It appears that for this data, QDA\n",
    "provides the best results of the methods that we have examined so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "KNN does not perform well on the Smarket data but it does often provide\n",
    "impressive results. As an example we will apply the KNN approach to the\n",
    "Caravan data set, which is part of the ISLR2 library. This data set includes 85\n",
    "predictors that measure demographic characteristics for 5,822 individuals.\n",
    "The response variable is Purchase, which indicates whether or not a given\n",
    "individual purchases a caravan insurance policy. In this data set, only 6%\n",
    "of people purchased caravan insurance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>5822</li>\n",
       "\t<li>86</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 5822\n",
       "\\item 86\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 5822\n",
       "2. 86\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 5822   86"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the dimension of the Caravan dataset\n",
    "\n",
    "dim(Caravan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following objects are masked from Caravan (pos = 7):\n",
      "\n",
      "    AAANHANG, ABESAUT, ABRAND, ABROM, ABYSTAND, AFIETS, AGEZONG,\n",
      "    AINBOED, ALEVEN, AMOTSCO, APERSAUT, APERSONG, APLEZIER, ATRACTOR,\n",
      "    AVRAAUT, AWABEDR, AWALAND, AWAOREG, AWAPART, AWERKT, AZEILPL,\n",
      "    MAANTHUI, MAUT0, MAUT1, MAUT2, MBERARBG, MBERARBO, MBERBOER,\n",
      "    MBERHOOG, MBERMIDD, MBERZELF, MFALLEEN, MFGEKIND, MFWEKIND,\n",
      "    MGEMLEEF, MGEMOMV, MGODGE, MGODOV, MGODPR, MGODRK, MHHUUR, MHKOOP,\n",
      "    MINK123M, MINK3045, MINK4575, MINK7512, MINKGEM, MINKM30, MKOOPKLA,\n",
      "    MOPLHOOG, MOPLLAAG, MOPLMIDD, MOSHOOFD, MOSTYPE, MRELGE, MRELOV,\n",
      "    MRELSA, MSKA, MSKB1, MSKB2, MSKC, MSKD, MZFONDS, MZPART, PAANHANG,\n",
      "    PBESAUT, PBRAND, PBROM, PBYSTAND, PFIETS, PGEZONG, PINBOED, PLEVEN,\n",
      "    PMOTSCO, PPERSAUT, PPERSONG, PPLEZIER, PTRACTOR, Purchase, PVRAAUT,\n",
      "    PWABEDR, PWALAND, PWAOREG, PWAPART, PWERKT, PZEILPL\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>No</dt>\n",
       "\t\t<dd>5474</dd>\n",
       "\t<dt>Yes</dt>\n",
       "\t\t<dd>348</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[No] 5474\n",
       "\\item[Yes] 348\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "No\n",
       ":   5474Yes\n",
       ":   348\n",
       "\n"
      ],
      "text/plain": [
       "  No  Yes \n",
       "5474  348 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attach(Caravan)\n",
    "\n",
    "summary(Purchase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the KNN classifier predicts the class of a given test observation by\n",
    "identifying the observations that are nearest to it, the scale of the variables\n",
    "matters. Variables that are on a large scale will have a much larger effect\n",
    "on the distance between the observations, and hence on the KNN classifier,\n",
    "than variables that are on a small scale.\n",
    "\n",
    "A good way to handle this problem is to standardize the data so that all\n",
    "standardize variables are given a mean of zero and a standard deviation of one. Then\n",
    "all variables will be on a comparable scale. The scale() function does just\n",
    "this. In standardizing the data, we exclude column 86, because that is the\n",
    "qualitative Purchase variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the Caravan Dataset\n",
    "\n",
    "standardized_X <- scale(Caravan[, -86])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every column of standardized.X has a standard deviation of one and\n",
    "a mean of zero.\n",
    "We now split the observations into a test set, containing the first 1,000\n",
    "observations, and a training set, containing the remaining observations.\n",
    "We fit a KNN model on the training data using K = 1, and evaluate its\n",
    "performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold-out the first 1000 rows for testing\n",
    "test_index <- 1:1000\n",
    "\n",
    "# Derrive the training and testing variables \n",
    "train_X <- standardized_X[-test_index, ]\n",
    "\n",
    "test_X <- standardized_X[test_index, ]\n",
    "\n",
    "# Derrive the test Response \n",
    "test_Y <- Purchase[test_index]\n",
    "\n",
    "train_Y <- Purchase[-test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        test_Y\n",
       "knn_pred  No Yes\n",
       "     No  874  49\n",
       "     Yes  67  10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.884"
      ],
      "text/latex": [
       "0.884"
      ],
      "text/markdown": [
       "0.884"
      ],
      "text/plain": [
       "[1] 0.884"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the k-NN with k = 1\n",
    "\n",
    "knn_pred <- knn(train_X, test_X, train_Y, k = 1)\n",
    "\n",
    "# Confusion Matrix\n",
    "table(knn_pred, test_Y)\n",
    "\n",
    "# Accuracy\n",
    "mean(knn_pred == test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector test is numeric, with values from 1 through 1, 000. Typing\n",
    "standardized.X[test, ] yields the submatrix of the data containing the\n",
    "observations whose indices range from 1 to 1, 000, whereas typing\n",
    "standardized.X[-test, ] yields the submatrix containing the observations\n",
    "whose indices do not range from 1 to 1, 000. The KNN error rate on the\n",
    "1,000 test observations is just under 12%. At first glance, this may appear\n",
    "to be fairly good. However, since only 6% of customers purchased\n",
    "insurance, we could get the error rate down to 6% by always predicting No\n",
    "regardless of the values of the predictors!\n",
    "\n",
    "Suppose that there is some non-trivial cost to trying to sell insurance\n",
    "to a given individual. For instance, perhaps a salesperson must visit each\n",
    "potential customer. If the company tries to sell insurance to a random\n",
    "selection of customers, then the success rate will be only 6 %, which may\n",
    "be far too low given the costs involved. Instead, the company would like\n",
    "to try to sell insurance only to customers who are likely to buy it. So the\n",
    "overall error rate is not of interest. Instead, the fraction of individuals that\n",
    "are correctly predicted to buy insurance is of interest.\n",
    "\n",
    "It turns out that KNN with K = 1 does far better than random guessing\n",
    "among the customers that are predicted to buy insurance. Among 77 such\n",
    "customers, 9, or 11.7 %, actually do purchase insurance. This is double the\n",
    "rate that one would obtain from random guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.12987012987013"
      ],
      "text/latex": [
       "0.12987012987013"
      ],
      "text/markdown": [
       "0.12987012987013"
      ],
      "text/plain": [
       "[1] 0.1298701"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fraction of Individuals likely to buy\n",
    "\n",
    "10 / (10 + 67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        test_Y\n",
       "knn_pred  No Yes\n",
       "     No  920  54\n",
       "     Yes  21   5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.925"
      ],
      "text/latex": [
       "0.925"
      ],
      "text/markdown": [
       "0.925"
      ],
      "text/plain": [
       "[1] 0.925"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using k = 3\n",
    "\n",
    "# Fit the k-NN with k = 1\n",
    "\n",
    "knn_pred <- knn(train_X, test_X, train_Y, k = 3)\n",
    "\n",
    "# Confusion Matrix\n",
    "table(knn_pred, test_Y)\n",
    "\n",
    "# Accuracy\n",
    "mean(knn_pred == test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using K = 3, the success rate increases to 19 %, and with K = 5 the rate is\n",
    "26.7 %. This is over four times the rate that results from random guessing.\n",
    "It appears that KNN is finding some real patterns in a difficult data set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.192307692307692"
      ],
      "text/latex": [
       "0.192307692307692"
      ],
      "text/markdown": [
       "0.192307692307692"
      ],
      "text/plain": [
       "[1] 0.1923077"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fraction of Individuals likely to buy\n",
    "\n",
    "5 / (21 + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector of k\n",
    "k_seq <- 1:20\n",
    "\n",
    "# Initialize an empty list of len(k)\n",
    "accuracy_list <- rep(NULL, length(k_seq))\n",
    "\n",
    "\n",
    "for (k in k_seq) {\n",
    "\n",
    "    # Fit the k-NN with the value of k\n",
    "    knn_pred <- knn(train_X, test_X, train_Y, k = k)\n",
    "\n",
    "    # Find the Accuracy\n",
    "    accuracy <- mean(knn_pred == test_Y)\n",
    "\n",
    "    # Append result to the list\n",
    "    accuracy_list <- append(accuracy_list, accuracy, after = (k - 1))\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>k_seq</th><th scope=col>accuracy_list</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1    </td><td>0.882</td></tr>\n",
       "\t<tr><td>2    </td><td>0.888</td></tr>\n",
       "\t<tr><td>3    </td><td>0.926</td></tr>\n",
       "\t<tr><td>4    </td><td>0.927</td></tr>\n",
       "\t<tr><td>5    </td><td>0.934</td></tr>\n",
       "\t<tr><td>6    </td><td>0.939</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " k\\_seq & accuracy\\_list\\\\\n",
       "\\hline\n",
       "\t 1     & 0.882\\\\\n",
       "\t 2     & 0.888\\\\\n",
       "\t 3     & 0.926\\\\\n",
       "\t 4     & 0.927\\\\\n",
       "\t 5     & 0.934\\\\\n",
       "\t 6     & 0.939\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| k_seq | accuracy_list |\n",
       "|---|---|\n",
       "| 1     | 0.882 |\n",
       "| 2     | 0.888 |\n",
       "| 3     | 0.926 |\n",
       "| 4     | 0.927 |\n",
       "| 5     | 0.934 |\n",
       "| 6     | 0.939 |\n",
       "\n"
      ],
      "text/plain": [
       "  k_seq accuracy_list\n",
       "1 1     0.882        \n",
       "2 2     0.888        \n",
       "3 3     0.926        \n",
       "4 4     0.927        \n",
       "5 5     0.934        \n",
       "6 6     0.939        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knn_results_df <- data.frame(k_seq, accuracy_list)\n",
    "\n",
    "head(knn_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following objects are masked from Bikeshare (pos = 5):\n",
      "\n",
      "    atemp, bikers, casual, day, holiday, hr, hum, mnth, registered,\n",
      "    season, temp, weathersit, weekday, windspeed, workingday\n",
      "\n",
      "The following objects are masked from Bikeshare (pos = 6):\n",
      "\n",
      "    atemp, bikers, casual, day, holiday, hr, hum, mnth, registered,\n",
      "    season, temp, weathersit, weekday, windspeed, workingday\n",
      "\n",
      "The following objects are masked from Bikeshare (pos = 7):\n",
      "\n",
      "    atemp, bikers, casual, day, holiday, hr, hum, mnth, registered,\n",
      "    season, temp, weathersit, weekday, windspeed, workingday\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>8645</li>\n",
       "\t<li>15</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 8645\n",
       "\\item 15\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 8645\n",
       "2. 15\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 8645   15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'season'</li>\n",
       "\t<li>'mnth'</li>\n",
       "\t<li>'day'</li>\n",
       "\t<li>'hr'</li>\n",
       "\t<li>'holiday'</li>\n",
       "\t<li>'weekday'</li>\n",
       "\t<li>'workingday'</li>\n",
       "\t<li>'weathersit'</li>\n",
       "\t<li>'temp'</li>\n",
       "\t<li>'atemp'</li>\n",
       "\t<li>'hum'</li>\n",
       "\t<li>'windspeed'</li>\n",
       "\t<li>'casual'</li>\n",
       "\t<li>'registered'</li>\n",
       "\t<li>'bikers'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'season'\n",
       "\\item 'mnth'\n",
       "\\item 'day'\n",
       "\\item 'hr'\n",
       "\\item 'holiday'\n",
       "\\item 'weekday'\n",
       "\\item 'workingday'\n",
       "\\item 'weathersit'\n",
       "\\item 'temp'\n",
       "\\item 'atemp'\n",
       "\\item 'hum'\n",
       "\\item 'windspeed'\n",
       "\\item 'casual'\n",
       "\\item 'registered'\n",
       "\\item 'bikers'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'season'\n",
       "2. 'mnth'\n",
       "3. 'day'\n",
       "4. 'hr'\n",
       "5. 'holiday'\n",
       "6. 'weekday'\n",
       "7. 'workingday'\n",
       "8. 'weathersit'\n",
       "9. 'temp'\n",
       "10. 'atemp'\n",
       "11. 'hum'\n",
       "12. 'windspeed'\n",
       "13. 'casual'\n",
       "14. 'registered'\n",
       "15. 'bikers'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"season\"     \"mnth\"       \"day\"        \"hr\"         \"holiday\"   \n",
       " [6] \"weekday\"    \"workingday\" \"weathersit\" \"temp\"       \"atemp\"     \n",
       "[11] \"hum\"        \"windspeed\"  \"casual\"     \"registered\" \"bikers\"    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Attach Bikeshare\n",
    "attach(Bikeshare)\n",
    "\n",
    "# Get the dimensions of the data\n",
    "dim(Bikeshare)\n",
    "\n",
    "# Get the column names\n",
    "names(Bikeshare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>2</th><th scope=col>3</th><th scope=col>4</th><th scope=col>5</th><th scope=col>6</th><th scope=col>7</th><th scope=col>8</th><th scope=col>9</th><th scope=col>10</th><th scope=col>11</th><th scope=col>12</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Jan</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>Feb</th><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>March</th><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>April</th><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>May</th><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>June</th><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>July</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>Aug</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>Sept</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>Oct</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>Nov</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>Dec</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllll}\n",
       "  & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12\\\\\n",
       "\\hline\n",
       "\tJan & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\tFeb & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\tMarch & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\tApril & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\tMay & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\tJune & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\tJuly & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\tAug & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n",
       "\tSept & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\\\\n",
       "\tOct & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\\n",
       "\tNov & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\\\\n",
       "\tDec & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| Jan | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| Feb | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| March | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| April | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| May | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| June | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
       "| July | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 |\n",
       "| Aug | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 |\n",
       "| Sept | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
       "| Oct | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 |\n",
       "| Nov | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |\n",
       "| Dec | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "      2 3 4 5 6 7 8 9 10 11 12\n",
       "Jan   0 0 0 0 0 0 0 0 0  0  0 \n",
       "Feb   1 0 0 0 0 0 0 0 0  0  0 \n",
       "March 0 1 0 0 0 0 0 0 0  0  0 \n",
       "April 0 0 1 0 0 0 0 0 0  0  0 \n",
       "May   0 0 0 1 0 0 0 0 0  0  0 \n",
       "June  0 0 0 0 1 0 0 0 0  0  0 \n",
       "July  0 0 0 0 0 1 0 0 0  0  0 \n",
       "Aug   0 0 0 0 0 0 1 0 0  0  0 \n",
       "Sept  0 0 0 0 0 0 0 1 0  0  0 \n",
       "Oct   0 0 0 0 0 0 0 0 1  0  0 \n",
       "Nov   0 0 0 0 0 0 0 0 0  1  0 \n",
       "Dec   0 0 0 0 0 0 0 0 0  0  1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>2</th><th scope=col>3</th><th scope=col>4</th><th scope=col>5</th><th scope=col>6</th><th scope=col>7</th><th scope=col>8</th><th scope=col>9</th><th scope=col>10</th><th scope=col>11</th><th scope=col>...</th><th scope=col>15</th><th scope=col>16</th><th scope=col>17</th><th scope=col>18</th><th scope=col>19</th><th scope=col>20</th><th scope=col>21</th><th scope=col>22</th><th scope=col>23</th><th scope=col>24</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>0</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>1</th><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>4</th><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>7</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>8</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>9</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>10</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>11</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>12</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>13</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>14</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>15</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>16</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>17</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>18</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>19</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>20</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>21</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>22</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>1  </td><td>0  </td></tr>\n",
       "\t<tr><th scope=row>23</th><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>...</td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>0  </td><td>1  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllll}\n",
       "  & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & ... & 15 & 16 & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24\\\\\n",
       "\\hline\n",
       "\t0 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t1 & 1   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t2 & 0   & 1   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t3 & 0   & 0   & 1   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t4 & 0   & 0   & 0   & 1   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t5 & 0   & 0   & 0   & 0   & 1   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t6 & 0   & 0   & 0   & 0   & 0   & 1   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t7 & 0   & 0   & 0   & 0   & 0   & 0   & 1   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t8 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 1   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t9 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 1   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t10 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 1   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t11 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t12 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t13 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t14 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 1   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t15 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 1   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t16 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 1   & 0   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t17 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 1   & 0   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t18 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 1   & 0   & 0   & 0   & 0   & 0  \\\\\n",
       "\t19 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 1   & 0   & 0   & 0   & 0  \\\\\n",
       "\t20 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 1   & 0   & 0   & 0  \\\\\n",
       "\t21 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 1   & 0   & 0  \\\\\n",
       "\t22 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 1   & 0  \\\\\n",
       "\t23 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & ... & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 1  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | ... | 15 | 16 | 17 | 18 | 19 | 20 | 21 | 22 | 23 | 24 |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 0 | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 1 | 1   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 2 | 0   | 1   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 3 | 0   | 0   | 1   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 4 | 0   | 0   | 0   | 1   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 5 | 0   | 0   | 0   | 0   | 1   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 6 | 0   | 0   | 0   | 0   | 0   | 1   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 7 | 0   | 0   | 0   | 0   | 0   | 0   | 1   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 8 | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 1   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 9 | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 1   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 10 | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 1   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 11 | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 12 | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 13 | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 14 | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 1   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 15 | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 1   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 16 | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 1   | 0   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 17 | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 1   | 0   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 18 | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 1   | 0   | 0   | 0   | 0   | 0   |\n",
       "| 19 | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 1   | 0   | 0   | 0   | 0   |\n",
       "| 20 | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 1   | 0   | 0   | 0   |\n",
       "| 21 | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 1   | 0   | 0   |\n",
       "| 22 | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 1   | 0   |\n",
       "| 23 | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | ... | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 1   |\n",
       "\n"
      ],
      "text/plain": [
       "   2 3 4 5 6 7 8 9 10 11 ... 15 16 17 18 19 20 21 22 23 24\n",
       "0  0 0 0 0 0 0 0 0 0  0  ... 0  0  0  0  0  0  0  0  0  0 \n",
       "1  1 0 0 0 0 0 0 0 0  0  ... 0  0  0  0  0  0  0  0  0  0 \n",
       "2  0 1 0 0 0 0 0 0 0  0  ... 0  0  0  0  0  0  0  0  0  0 \n",
       "3  0 0 1 0 0 0 0 0 0  0  ... 0  0  0  0  0  0  0  0  0  0 \n",
       "4  0 0 0 1 0 0 0 0 0  0  ... 0  0  0  0  0  0  0  0  0  0 \n",
       "5  0 0 0 0 1 0 0 0 0  0  ... 0  0  0  0  0  0  0  0  0  0 \n",
       "6  0 0 0 0 0 1 0 0 0  0  ... 0  0  0  0  0  0  0  0  0  0 \n",
       "7  0 0 0 0 0 0 1 0 0  0  ... 0  0  0  0  0  0  0  0  0  0 \n",
       "8  0 0 0 0 0 0 0 1 0  0  ... 0  0  0  0  0  0  0  0  0  0 \n",
       "9  0 0 0 0 0 0 0 0 1  0  ... 0  0  0  0  0  0  0  0  0  0 \n",
       "10 0 0 0 0 0 0 0 0 0  1  ... 0  0  0  0  0  0  0  0  0  0 \n",
       "11 0 0 0 0 0 0 0 0 0  0  ... 0  0  0  0  0  0  0  0  0  0 \n",
       "12 0 0 0 0 0 0 0 0 0  0  ... 0  0  0  0  0  0  0  0  0  0 \n",
       "13 0 0 0 0 0 0 0 0 0  0  ... 0  0  0  0  0  0  0  0  0  0 \n",
       "14 0 0 0 0 0 0 0 0 0  0  ... 1  0  0  0  0  0  0  0  0  0 \n",
       "15 0 0 0 0 0 0 0 0 0  0  ... 0  1  0  0  0  0  0  0  0  0 \n",
       "16 0 0 0 0 0 0 0 0 0  0  ... 0  0  1  0  0  0  0  0  0  0 \n",
       "17 0 0 0 0 0 0 0 0 0  0  ... 0  0  0  1  0  0  0  0  0  0 \n",
       "18 0 0 0 0 0 0 0 0 0  0  ... 0  0  0  0  1  0  0  0  0  0 \n",
       "19 0 0 0 0 0 0 0 0 0  0  ... 0  0  0  0  0  1  0  0  0  0 \n",
       "20 0 0 0 0 0 0 0 0 0  0  ... 0  0  0  0  0  0  1  0  0  0 \n",
       "21 0 0 0 0 0 0 0 0 0  0  ... 0  0  0  0  0  0  0  1  0  0 \n",
       "22 0 0 0 0 0 0 0 0 0  0  ... 0  0  0  0  0  0  0  0  1  0 \n",
       "23 0 0 0 0 0 0 0 0 0  0  ... 0  0  0  0  0  0  0  0  0  1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "contrasts(Bikeshare$mnth) <- contr.treatment(12)\n",
    "\n",
    "contrasts(Bikeshare$hr) <- contr.treatment(24)\n",
    "\n",
    "contrasts(Bikeshare$mnth)\n",
    "\n",
    "contrasts(Bikeshare$hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in summary.lm(lin_reg_fit):\n",
      "\"essentially perfect fit: summary may be unreliable\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = bikers ~ ., data = Bikeshare)\n",
       "\n",
       "Residuals:\n",
       "       Min         1Q     Median         3Q        Max \n",
       "-4.188e-12 -2.170e-14 -1.100e-15  2.040e-14  4.427e-12 \n",
       "\n",
       "Coefficients:\n",
       "                            Estimate Std. Error    t value Pr(>|t|)    \n",
       "(Intercept)                9.284e-14  9.959e-15  9.322e+00  < 2e-16 ***\n",
       "season                    -2.254e-14  2.364e-15 -9.532e+00  < 2e-16 ***\n",
       "mnth2                      3.002e-15  6.909e-15  4.340e-01 0.663938    \n",
       "mnth3                      2.052e-14  9.518e-15  2.156e+00 0.031142 *  \n",
       "mnth4                      4.717e-15  1.332e-14  3.540e-01 0.723195    \n",
       "mnth5                     -3.590e-14  1.699e-14 -2.113e+00 0.034625 *  \n",
       "mnth6                      9.765e-15  2.093e-14  4.670e-01 0.640775    \n",
       "mnth7                     -6.135e-14  2.495e-14 -2.459e+00 0.013943 *  \n",
       "mnth8                      1.075e-13  2.841e-14  3.783e+00 0.000156 ***\n",
       "mnth9                      1.574e-13  3.211e-14  4.901e+00 9.69e-07 ***\n",
       "mnth10                     5.883e-15  3.618e-14  1.630e-01 0.870847    \n",
       "mnth11                     2.058e-14  3.999e-14  5.150e-01 0.606754    \n",
       "mnth12                     1.518e-14  4.348e-14  3.490e-01 0.726935    \n",
       "day                       -1.494e-16  1.285e-16 -1.163e+00 0.244899    \n",
       "hr2                       -9.765e-14  7.743e-15 -1.261e+01  < 2e-16 ***\n",
       "hr3                       -1.145e-13  7.794e-15 -1.469e+01  < 2e-16 ***\n",
       "hr4                        7.180e-15  7.863e-15  9.130e-01 0.361214    \n",
       "hr5                       -7.558e-15  7.903e-15 -9.560e-01 0.338956    \n",
       "hr6                       -8.144e-14  7.804e-15 -1.044e+01  < 2e-16 ***\n",
       "hr7                       -2.199e-14  7.772e-15 -2.829e+00 0.004681 ** \n",
       "hr8                       -7.007e-14  8.061e-15 -8.693e+00  < 2e-16 ***\n",
       "hr9                       -1.701e-13  8.718e-15 -1.952e+01  < 2e-16 ***\n",
       "hr10                      -1.047e-13  7.979e-15 -1.312e+01  < 2e-16 ***\n",
       "hr11                      -6.138e-14  7.886e-15 -7.784e+00 7.86e-15 ***\n",
       "hr12                      -3.322e-14  7.990e-15 -4.157e+00 3.25e-05 ***\n",
       "hr13                       9.672e-15  8.135e-15  1.189e+00 0.234524    \n",
       "hr14                      -2.337e-15  8.190e-15 -2.850e-01 0.775431    \n",
       "hr15                      -5.839e-14  8.215e-15 -7.108e+00 1.27e-12 ***\n",
       "hr16                      -5.868e-14  8.228e-15 -7.132e+00 1.07e-12 ***\n",
       "hr17                      -8.733e-14  8.351e-15 -1.046e+01  < 2e-16 ***\n",
       "hr18                       2.208e-15  9.078e-15  2.430e-01 0.807873    \n",
       "hr19                      -2.036e-13  8.890e-15 -2.290e+01  < 2e-16 ***\n",
       "hr20                      -1.718e-13  8.295e-15 -2.071e+01  < 2e-16 ***\n",
       "hr21                      -1.131e-13  7.995e-15 -1.414e+01  < 2e-16 ***\n",
       "hr22                      -1.870e-14  7.852e-15 -2.382e+00 0.017242 *  \n",
       "hr23                       3.437e-15  7.782e-15  4.420e-01 0.658719    \n",
       "hr24                       1.125e-14  7.737e-15  1.454e+00 0.146007    \n",
       "holiday                    5.813e-16  7.163e-15  8.100e-02 0.935318    \n",
       "weekday                    1.059e-15  5.650e-16  1.874e+00 0.060987 .  \n",
       "workingday                -2.746e-14  3.021e-15 -9.089e+00  < 2e-16 ***\n",
       "weathersitcloudy/misty    -4.728e-15  2.823e-15 -1.675e+00 0.093986 .  \n",
       "weathersitlight rain/snow -1.379e-14  4.564e-15 -3.022e+00 0.002520 ** \n",
       "weathersitheavy rain/snow -9.300e-14  1.042e-13 -8.920e-01 0.372358    \n",
       "temp                       1.552e-13  5.314e-14  2.921e+00 0.003496 ** \n",
       "atemp                     -1.199e-13  5.591e-14 -2.145e+00 0.031985 *  \n",
       "hum                        3.419e-14  8.032e-15  4.257e+00 2.09e-05 ***\n",
       "windspeed                 -3.705e-14  1.045e-14 -3.546e+00 0.000393 ***\n",
       "casual                     1.000e+00  4.865e-17  2.055e+16  < 2e-16 ***\n",
       "registered                 1.000e+00  1.984e-17  5.040e+16  < 2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 1.039e-13 on 8596 degrees of freedom\n",
       "Multiple R-squared:      1,\tAdjusted R-squared:      1 \n",
       "F-statistic: 2.986e+32 on 48 and 8596 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the linear regression model\n",
    "lin_reg_fit <- lm(bikers ~ ., data = Bikeshare)\n",
    "\n",
    "# Get the summary\n",
    "summary(lin_reg_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n",
       "    data = Bikeshare)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-299.00  -45.70   -6.23   41.08  425.29 \n",
       "\n",
       "Coefficients:\n",
       "                          Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)                -68.632      5.307 -12.932  < 2e-16 ***\n",
       "mnth2                        6.845      4.287   1.597 0.110398    \n",
       "mnth3                       16.551      4.301   3.848 0.000120 ***\n",
       "mnth4                       41.425      4.972   8.331  < 2e-16 ***\n",
       "mnth5                       72.557      5.641  12.862  < 2e-16 ***\n",
       "mnth6                       67.819      6.544  10.364  < 2e-16 ***\n",
       "mnth7                       45.324      7.081   6.401 1.63e-10 ***\n",
       "mnth8                       53.243      6.640   8.019 1.21e-15 ***\n",
       "mnth9                       66.678      5.925  11.254  < 2e-16 ***\n",
       "mnth10                      75.834      4.950  15.319  < 2e-16 ***\n",
       "mnth11                      60.310      4.610  13.083  < 2e-16 ***\n",
       "mnth12                      46.458      4.271  10.878  < 2e-16 ***\n",
       "hr2                        -14.579      5.699  -2.558 0.010536 *  \n",
       "hr3                        -21.579      5.733  -3.764 0.000168 ***\n",
       "hr4                        -31.141      5.778  -5.389 7.26e-08 ***\n",
       "hr5                        -36.908      5.802  -6.361 2.11e-10 ***\n",
       "hr6                        -24.135      5.737  -4.207 2.61e-05 ***\n",
       "hr7                         20.600      5.704   3.612 0.000306 ***\n",
       "hr8                        120.093      5.693  21.095  < 2e-16 ***\n",
       "hr9                        223.662      5.690  39.310  < 2e-16 ***\n",
       "hr10                       120.582      5.693  21.182  < 2e-16 ***\n",
       "hr11                        83.801      5.705  14.689  < 2e-16 ***\n",
       "hr12                       105.423      5.722  18.424  < 2e-16 ***\n",
       "hr13                       137.284      5.740  23.916  < 2e-16 ***\n",
       "hr14                       136.036      5.760  23.617  < 2e-16 ***\n",
       "hr15                       126.636      5.776  21.923  < 2e-16 ***\n",
       "hr16                       132.087      5.780  22.852  < 2e-16 ***\n",
       "hr17                       178.521      5.772  30.927  < 2e-16 ***\n",
       "hr18                       296.267      5.749  51.537  < 2e-16 ***\n",
       "hr19                       269.441      5.736  46.976  < 2e-16 ***\n",
       "hr20                       186.256      5.714  32.596  < 2e-16 ***\n",
       "hr21                       125.549      5.704  22.012  < 2e-16 ***\n",
       "hr22                        87.554      5.693  15.378  < 2e-16 ***\n",
       "hr23                        59.123      5.689  10.392  < 2e-16 ***\n",
       "hr24                        26.838      5.688   4.719 2.41e-06 ***\n",
       "workingday                   1.270      1.784   0.711 0.476810    \n",
       "temp                       157.209     10.261  15.321  < 2e-16 ***\n",
       "weathersitcloudy/misty     -12.890      1.964  -6.562 5.60e-11 ***\n",
       "weathersitlight rain/snow  -66.494      2.965 -22.425  < 2e-16 ***\n",
       "weathersitheavy rain/snow -109.745     76.667  -1.431 0.152341    \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 76.5 on 8605 degrees of freedom\n",
       "Multiple R-squared:  0.6745,\tAdjusted R-squared:  0.6731 \n",
       "F-statistic: 457.3 on 39 and 8605 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the linear regression model\n",
    "lin_reg_fit <- lm(bikers ~ mnth + hr + workingday + temp +\n",
    "weathersit, data = Bikeshare)\n",
    "\n",
    "# Get the summary\n",
    "summary(lin_reg_fit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to space constraints, we truncate the output of summary(lin_reg_fit). In\n",
    "mod.lm, the first level of hr (0) and mnth (Jan) are treated as the baseline\n",
    "values, and so no coefficient estimates are provided for them: implicitly,\n",
    "their coefficient estimates are zero, and all other levels are measured relative\n",
    "to these baselines. For example, the Feb coefficient of 6.845 signifies that,\n",
    "holding all other variables constant, there are on average about 7 more\n",
    "riders in February than in January. Similarly there are about 16.5 more\n",
    "riders in March than in January."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n",
       "    data = Bikeshare)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-299.00  -45.70   -6.23   41.08  425.29 \n",
       "\n",
       "Coefficients:\n",
       "                           Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)                 73.5974     5.1322  14.340  < 2e-16 ***\n",
       "mnth1                      -46.0871     4.0855 -11.281  < 2e-16 ***\n",
       "mnth2                      -39.2419     3.5391 -11.088  < 2e-16 ***\n",
       "mnth3                      -29.5357     3.1552  -9.361  < 2e-16 ***\n",
       "mnth4                       -4.6622     2.7406  -1.701  0.08895 .  \n",
       "mnth5                       26.4700     2.8508   9.285  < 2e-16 ***\n",
       "mnth6                       21.7317     3.4651   6.272 3.75e-10 ***\n",
       "mnth7                       -0.7626     3.9084  -0.195  0.84530    \n",
       "mnth8                        7.1560     3.5347   2.024  0.04295 *  \n",
       "mnth9                       20.5912     3.0456   6.761 1.46e-11 ***\n",
       "mnth10                      29.7472     2.6995  11.019  < 2e-16 ***\n",
       "mnth11                      14.2229     2.8604   4.972 6.74e-07 ***\n",
       "hr1                        -96.1420     3.9554 -24.307  < 2e-16 ***\n",
       "hr2                       -110.7213     3.9662 -27.916  < 2e-16 ***\n",
       "hr3                       -117.7212     4.0165 -29.310  < 2e-16 ***\n",
       "hr4                       -127.2828     4.0808 -31.191  < 2e-16 ***\n",
       "hr5                       -133.0495     4.1168 -32.319  < 2e-16 ***\n",
       "hr6                       -120.2775     4.0370 -29.794  < 2e-16 ***\n",
       "hr7                        -75.5424     3.9916 -18.925  < 2e-16 ***\n",
       "hr8                         23.9511     3.9686   6.035 1.65e-09 ***\n",
       "hr9                        127.5199     3.9500  32.284  < 2e-16 ***\n",
       "hr10                        24.4399     3.9360   6.209 5.57e-10 ***\n",
       "hr11                       -12.3407     3.9361  -3.135  0.00172 ** \n",
       "hr12                         9.2814     3.9447   2.353  0.01865 *  \n",
       "hr13                        41.1417     3.9571  10.397  < 2e-16 ***\n",
       "hr14                        39.8939     3.9750  10.036  < 2e-16 ***\n",
       "hr15                        30.4940     3.9910   7.641 2.39e-14 ***\n",
       "hr16                        35.9445     3.9949   8.998  < 2e-16 ***\n",
       "hr17                        82.3786     3.9883  20.655  < 2e-16 ***\n",
       "hr18                       200.1249     3.9638  50.488  < 2e-16 ***\n",
       "hr19                       173.2989     3.9561  43.806  < 2e-16 ***\n",
       "hr20                        90.1138     3.9400  22.872  < 2e-16 ***\n",
       "hr21                        29.4071     3.9362   7.471 8.74e-14 ***\n",
       "hr22                        -8.5883     3.9332  -2.184  0.02902 *  \n",
       "hr23                       -37.0194     3.9344  -9.409  < 2e-16 ***\n",
       "workingday                   1.2696     1.7845   0.711  0.47681    \n",
       "temp                       157.2094    10.2612  15.321  < 2e-16 ***\n",
       "weathersitcloudy/misty     -12.8903     1.9643  -6.562 5.60e-11 ***\n",
       "weathersitlight rain/snow  -66.4944     2.9652 -22.425  < 2e-16 ***\n",
       "weathersitheavy rain/snow -109.7446    76.6674  -1.431  0.15234    \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 76.5 on 8605 degrees of freedom\n",
       "Multiple R-squared:  0.6745,\tAdjusted R-squared:  0.6731 \n",
       "F-statistic: 457.3 on 39 and 8605 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Changing the baseline for coded variables\n",
    "contrasts(Bikeshare$hr) <- contr.sum(24)\n",
    "\n",
    "contrasts(Bikeshare$mnth) <- contr.sum(12)\n",
    "\n",
    "# Fit the linear regression model\n",
    "lin_reg_fit2 <- lm(bikers ~ mnth + hr + workingday + temp +\n",
    "weathersit, data = Bikeshare)\n",
    "\n",
    "# Get the summary\n",
    "summary(lin_reg_fit2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lin_reg_fit2, a coefficient\n",
    "estimate is reported for all but the last level of hr and mnth. Importantly,\n",
    "in lin_reg_fit2, the coefficient estimate for the last level of mnth is not zero:\n",
    "instead, it equals the negative of the sum of the coefficient estimates for\n",
    "all of the other levels. Similarly, in mod.lm2, the coefficient estimate for the\n",
    "last level of hr is the negative of the sum of the coefficient estimates for\n",
    "all of the other levels. This means that the coefficients of hr and mnth in\n",
    "lin_reg_fit2 will always sum to zero, and can be interpreted as the difference\n",
    "from the mean level. For example, the coefficient for January of −46.087\n",
    "indicates that, holding all other variables constant, there are typically 46\n",
    "fewer riders in January relative to the yearly average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n",
       "    family = poisson, data = Bikeshare)\n",
       "\n",
       "Deviance Residuals: \n",
       "     Min        1Q    Median        3Q       Max  \n",
       "-20.7574   -3.3441   -0.6549    2.6999   21.9628  \n",
       "\n",
       "Coefficients:\n",
       "                           Estimate Std. Error  z value Pr(>|z|)    \n",
       "(Intercept)                4.118245   0.006021  683.964  < 2e-16 ***\n",
       "mnth1                     -0.670170   0.005907 -113.445  < 2e-16 ***\n",
       "mnth2                     -0.444124   0.004860  -91.379  < 2e-16 ***\n",
       "mnth3                     -0.293733   0.004144  -70.886  < 2e-16 ***\n",
       "mnth4                      0.021523   0.003125    6.888 5.66e-12 ***\n",
       "mnth5                      0.240471   0.002916   82.462  < 2e-16 ***\n",
       "mnth6                      0.223235   0.003554   62.818  < 2e-16 ***\n",
       "mnth7                      0.103617   0.004125   25.121  < 2e-16 ***\n",
       "mnth8                      0.151171   0.003662   41.281  < 2e-16 ***\n",
       "mnth9                      0.233493   0.003102   75.281  < 2e-16 ***\n",
       "mnth10                     0.267573   0.002785   96.091  < 2e-16 ***\n",
       "mnth11                     0.150264   0.003180   47.248  < 2e-16 ***\n",
       "hr1                       -0.754386   0.007879  -95.744  < 2e-16 ***\n",
       "hr2                       -1.225979   0.009953 -123.173  < 2e-16 ***\n",
       "hr3                       -1.563147   0.011869 -131.702  < 2e-16 ***\n",
       "hr4                       -2.198304   0.016424 -133.846  < 2e-16 ***\n",
       "hr5                       -2.830484   0.022538 -125.586  < 2e-16 ***\n",
       "hr6                       -1.814657   0.013464 -134.775  < 2e-16 ***\n",
       "hr7                       -0.429888   0.006896  -62.341  < 2e-16 ***\n",
       "hr8                        0.575181   0.004406  130.544  < 2e-16 ***\n",
       "hr9                        1.076927   0.003563  302.220  < 2e-16 ***\n",
       "hr10                       0.581769   0.004286  135.727  < 2e-16 ***\n",
       "hr11                       0.336852   0.004720   71.372  < 2e-16 ***\n",
       "hr12                       0.494121   0.004392  112.494  < 2e-16 ***\n",
       "hr13                       0.679642   0.004069  167.040  < 2e-16 ***\n",
       "hr14                       0.673565   0.004089  164.722  < 2e-16 ***\n",
       "hr15                       0.624910   0.004178  149.570  < 2e-16 ***\n",
       "hr16                       0.653763   0.004132  158.205  < 2e-16 ***\n",
       "hr17                       0.874301   0.003784  231.040  < 2e-16 ***\n",
       "hr18                       1.294635   0.003254  397.848  < 2e-16 ***\n",
       "hr19                       1.212281   0.003321  365.084  < 2e-16 ***\n",
       "hr20                       0.914022   0.003700  247.065  < 2e-16 ***\n",
       "hr21                       0.616201   0.004191  147.045  < 2e-16 ***\n",
       "hr22                       0.364181   0.004659   78.173  < 2e-16 ***\n",
       "hr23                       0.117493   0.005225   22.488  < 2e-16 ***\n",
       "workingday                 0.014665   0.001955    7.502 6.27e-14 ***\n",
       "temp                       0.785292   0.011475   68.434  < 2e-16 ***\n",
       "weathersitcloudy/misty    -0.075231   0.002179  -34.528  < 2e-16 ***\n",
       "weathersitlight rain/snow -0.575800   0.004058 -141.905  < 2e-16 ***\n",
       "weathersitheavy rain/snow -0.926287   0.166782   -5.554 2.79e-08 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for poisson family taken to be 1)\n",
       "\n",
       "    Null deviance: 1052921  on 8644  degrees of freedom\n",
       "Residual deviance:  228041  on 8605  degrees of freedom\n",
       "AIC: 281159\n",
       "\n",
       "Number of Fisher Scoring iterations: 5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fitting a Poisson Regression\n",
    "\n",
    "pois_reg_fit <- glm(bikers ∼ mnth + hr + workingday + temp + weathersit,\n",
    "data = Bikeshare, family = poisson)\n",
    "\n",
    "summary(pois_reg_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(predict(lin_reg_fit2), predict(pois_reg_fit, type = \"response\"))\n",
    "\n",
    "abline(0, 1, col = 2, lwd = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The predictions from the Poisson regression model are correlated with those\n",
    "from the linear model; however, the former are non-negative. As a result\n",
    "the Poisson regression predictions tend to be larger than those from the\n",
    "linear model for either very low or very high levels of ridership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "python397jvsc74a57bd015a99dc70f55bf54f5bf01a7bb523eaef6639a4e03a216c1285b710bbab4b128"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
